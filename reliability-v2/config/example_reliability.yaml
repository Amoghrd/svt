reliability:
  kubeconfig: <path_to_kubeconfig>
  users:
    - kubeadmin_password: <path_to_kubeadmin-password>
    - user_file: <path_to_users.spec>

  appTemplates:
    - template: cakephp-mysql-persistent
    - template: nodejs-postgresql-persistent
    - template: django-psql-persistent
    - template: rails-pgsql-persistent
    - template: dancer-mysql-persistent

  # The max number of projects is limited by the size of cluster.
  # For 3 nodes m5.xlarge cluster, 25 to 30 projects are recomended.
  # For 5 nodes m5.xlarge cluster, 60 projects are recomended.
  # The max number of projects = groups x users x new_project
  groups:
    - name: admin
      user_name: kubeadmin # username
      loops: forever # run group for loops times. integer > 0 or 'forever', default is 1.
      trigger: 600 # wait trigger seconds between each loop
      jitter: 60 # randomly start the users in this group in trigger seconds. Default is 0.
      interval: 10 # wait interval seconds between tasks.
      tasks: 
        - func check_operators
        - oc get project -l purpose=reliability
        - func check_nodes
        - kubectl get pods -A -o wide | egrep -v "Completed|Running"
        # Run test case as scripts. KUBECONFIG of the current user is set as env variable by reliability-v2. 
        #- . <path to /content/create-delete-pod-ensure-service.sh>
        
    - name: dev-test
      user_name: testuser- # if user_start and user_end exist, this will be username prefix
      user_start: 0 # user_start is inclusive, start with testuser-0 in the users.spec file
      user_end: 15 # user_end is exclusive, end with testuser-14 in the users.spec file
      loops: forever
      trigger: 60
      jitter: 600 # randomly start the users in this group in 10 minutes
      interval: 10
      tasks:
        - func delete_all_projects # clear all projects
        - func new_project 2 # new 2 projects
        # If network policy is planed in the test, uncomment the following line
        #- func apply 2 "<path to /reliability-v2/networkpolicy/allow-same-namespace.yaml>" # Apply network policy to 2 projects
        - func check_all_projects # check all project under this user
        - func new_app 2 # new app in 2 namespaces
        - func load_app 2 10 # load apps in 2 namespaces with 10 clients for each
        - func build 1 # build app in 1 namespace
        - func check_pods 2 # check pods in 2 namespaces 
        - func delete_project 2 # delete project in 2 namespaces

    - name: dev-prod
      user_name: testuser- 
      user_start: 15 # user_start is inclusive, start with testuser-15 in users.spec
      user_end: 16 # user_end is exclusive, end with testuser-15 in users.spec
      loops: forever
      trigger: 600
      jitter: 1200 # randomly start the users in this group in 20 minutes
      interval: 600
      pre_tasks: 
          - func delete_all_projects # clear all projects
          - func new_project 2 # new 2 projects
          - func new_app 2 # new app in 2 namespaces
      tasks:
        - func load_app 2 10 # load apps in 2 namespaces with 10 clients for each
        - func scale_deployment 2 2 # scale app in 2 namespaces to 2 replicas
        - func scale_deployment 2 1 # scale app in 2 namespaces to 1 replicas
      post_tasks: 
        - func delete_project 2

  cerberusIntegration:
    # start cerberus https://github.com/cloud-bulldozer/cerberus before starting reliabiity test.
    cerberus_enable: False
    # if cerberus_enable is false, the following 2 items are ignored.
    cerberus_api: "http://0.0.0.0:8080"
    # action to take when cerberus status is False, valid data: pause/halt/continue
    cerberus_fail_action: pause
  
  slackIntegration:
    slack_enable: False
    # the ID in the example is the id of slack channel #ocp-qe-reliability-monitoring.
    slack_channel: C0266JJ4XM5
    # slack_member is optional. If provided, the notification message will @ you. 
    # you must be a member of the slack channel to receive the notification.
    slack_member: <Your slack member id>

  krakenIntegration:
    kraken_enable: False
    # supported Kraken scenarios: https://github.com/cloud-bulldozer/kraken-hub/blob/main/README.md#supported-chaos-scenarios
    # pod-scenarios, container-scenarios, node-scenarios, zone-outages, time-scenarios, 
    # node-cpu-hog, node-memory-hog, node-io-hog
    # Please specify the parameters for each scenario. e.g. For pod-scenarios,
    # refer to https://github.com/cloud-bulldozer/kraken-hub/blob/main/docs/pod-scenarios.md#supported-parameters
    kraken_scenarios: 
      - name: pod-scenarios_etcd
        scenario: "pod-scenarios"
        interval_unit: minutes # weeks,days,hours,minutes,seconds
        interval_number: 8
        # start_date: "2021-10-28 17:36:00" # Optional. format: 2021-10-20 10:00:00.
        # end_date: "2021-10-28 17:50:00" # Optional.
        # timezone: "Asia/Shanghai" # Optional. e.g. US/Eastern. Default is "UTC
      - name: pod-scenarios_monitoring
        scenario: "pod-scenarios"
        interval_unit: minutes # weeks,days,hours,minutes,seconds
        interval_number: 10
        parameters:
          NAMESPACE: openshift-monitoring
          POD_LABEL: app.kubernetes.io/component=prometheus
          EXPECTED_POD_COUNT: 2
      - name: node-scenarios_workerstopstart
        scenario: "node-scenarios"
        interval_unit: minutes
        interval_number: 12
        parameters:
          AWS_DEFAULT_REGION: us-east-2
          AWS_ACCESS_KEY_ID: xxxx
          AWS_SECRET_ACCESS_KEY: xxxx
          CLOUD_TYPE: aws
